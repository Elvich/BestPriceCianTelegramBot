
import asyncio
import aiohttp
from aiohttp_socks import ProxyConnector
from bs4 import BeautifulSoup
import re
import random
import logging
from typing import List, Optional, Dict, Any
from core.config import config
from .proxy_manager import ProxyManager

logger = logging.getLogger(__name__)

class CianParser:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π –æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ –¶–∏–∞–Ω.
    –° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ä–æ—Ç–∞—Ü–∏–∏ –ø—Ä–æ–∫—Å–∏.
    """
    
    def __init__(self, session: Optional[aiohttp.ClientSession] = None):
        self.session = session
        self.proxy_manager = ProxyManager()
        self.current_proxy: Optional[str] = None
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä –Ω–∞ macOS –¥–ª—è –æ–±—Ö–æ–¥–∞ –±–ª–æ–∫–∏—Ä–æ–≤–æ–∫ (Chrome 131)
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept-Encoding": "gzip, deflate, br",
            "Sec-Ch-Ua": '"Not A(Brand";v="99", "Google Chrome";v="131", "Chromium";v="131"',
            "Sec-Ch-Ua-Mobile": "?0",
            "Sec-Ch-Ua-Platform": '"macOS"',
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Upgrade-Insecure-Requests": "1"
        }

    async def initialize(self):
        """Initializes the proxy manager"""
        await self.proxy_manager.initialize()

    async def _get_current_session(self) -> aiohttp.ClientSession:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â—É—é —Å–µ—Å—Å–∏—é, —Å–æ–∑–¥–∞–≤–∞—è –Ω–æ–≤—É—é —Å –ø—Ä–æ–∫—Å–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        if self.session and not self.session.closed:
            return self.session
            
        # Get a new proxy
        self.current_proxy = await self.proxy_manager.get_proxy()
        logger.info(f"Using proxy: {self.current_proxy}")
        
        connector = None
        if self.current_proxy and self.current_proxy.startswith('socks'):
            connector = ProxyConnector.from_url(self.current_proxy)
            
        self.session = aiohttp.ClientSession(
            headers=self.headers, 
            connector=connector,
            timeout=aiohttp.ClientTimeout(total=config.REQUEST_TIMEOUT)
        )
        return self.session

    async def _reset_session(self):
        """Resets the current session (used on error)"""
        if self.session:
            await self.session.close()
        self.session = None

    async def close(self):
        """Zakryvaet sessiyu"""
        if self.session and not self.session.closed:
            await self.session.close()

    async def _fetch_page_content(self, url: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –∏ —Ä–æ—Ç–∞—Ü–∏–µ–π –ø—Ä–æ–∫—Å–∏"""
        max_retries = config.MAX_RETRIES
        
        for attempt in range(max_retries):
            session = await self._get_current_session()
            
            try:
                # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ø—Ä–æ–∫—Å–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (–µ—Å–ª–∏ HTTP)
                proxy_url = None
                if self.current_proxy and not self.current_proxy.startswith('socks'):
                    proxy_url = self.current_proxy
                
                async with session.get(url, proxy=proxy_url, ssl=config.VERIFY_SSL) as response:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É –∏–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
                    if response.status == 403 or response.status == 429:
                        logger.warning(f"üö´ Blocked/Captcha ({response.status}) on {url} with {self.current_proxy}")
                        if self.current_proxy:
                            await self.proxy_manager.report_bad_proxy(self.current_proxy)
                        await self._reset_session()
                        continue
                        
                    if response.status == 200:
                        content = await response.text()
                        
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                        if any(kw in content.lower() for kw in ['captcha', 'security check', '–º—ã —Ö–æ—Ç–∏–º —É–±–µ–¥–∏—Ç—å—Å—è']):
                             logger.warning(f"üö´ Captcha detected in content on {url} with {self.current_proxy}")
                             if self.current_proxy:
                                 await self.proxy_manager.report_bad_proxy(self.current_proxy)
                             await self._reset_session()
                             continue
                             
                        return content
                    else:
                        logger.warning(f"‚ö†Ô∏è Status {response.status} loading {url}")
                        return None
                        
            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                logger.error(f"‚ùå Network error on {url} (Attempt {attempt+1}/{max_retries}): {e}")
                if self.current_proxy:
                    await self.proxy_manager.report_bad_proxy(self.current_proxy)
                await self._reset_session()
                # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º (—É–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ —à–∞–Ω—Å–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)
                await asyncio.sleep(random.uniform(2, 5))
                
            except Exception as e:
                logger.error(f"‚ùå Unexpected error on {url}: {e}")
                await self._reset_session()
                
        logger.error(f"‚ùå Failed to fetch {url} after {max_retries} attempts")
        return None

    def _extract_metro_info(self, soup: BeautifulSoup) -> List[Dict[str, str]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–µ—Ç—Ä–æ (—Å—Ç–∞–Ω—Ü–∏–∏ –∏ –≤—Ä–µ–º—è)"""
        metro_blocks = soup.find_all('li', {'data-name': 'UndergroundItem'})
        if not metro_blocks:
            metro_blocks = soup.find_all('div', class_=re.compile(r'--underground-name--'))
        
        station_info = []
        for block in metro_blocks:
            try:
                name_elem = block.find('a', class_=re.compile(r'--underground-link--'))
                if not name_elem:
                    name_elem = block
                
                name = name_elem.get_text(strip=True) if name_elem else "Unknown"
                
                time_elem = block.find('span', class_=re.compile(r'--underground-time--'))
                if not time_elem:
                    parent = block.find_parent('li')
                    if parent:
                        time_elem = parent.find('span', class_=re.compile(r'--underground-time--'))
                
                time_val = time_elem.get_text(strip=True) if time_elem else ""
                station_info.append({'station': name, 'time': time_val})
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –º–µ—Ç—Ä–æ: {e}")
                continue
                
        return station_info

    def _extract_floor_info(self, soup: BeautifulSoup) -> tuple[Optional[int], Optional[int]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–∞–∂–µ –∏ —ç—Ç–∞–∂–Ω–æ—Å—Ç–∏"""
        floor_text = soup.find('div', string=re.compile(r'–≠—Ç–∞–∂'))
        if floor_text:
            value_elem = floor_text.find_next_sibling('div')
            if value_elem:
                val = value_elem.get_text(strip=True)
                match = re.search(r'(\d+)\s*–∏–∑\s*(\d+)', val)
                if match:
                    return int(match.group(1)), int(match.group(2))
                    
        fact_items = soup.find_all('div', {'data-name': 'OfferFactItem'})
        for item in fact_items:
            title = item.find('div', class_=re.compile(r'--title--'))
            if title and '—ç—Ç–∞–∂' in title.get_text(strip=True).lower():
                value = item.find('div', class_=re.compile(r'--value--'))
                if value:
                    val = value.get_text(strip=True)
                    match = re.search(r'(\d+)\s*–∏–∑\s*(\d+)', val)
                    if match:
                        return int(match.group(1)), int(match.group(2))

        return None, None

    def _extract_views(self, soup: BeautifulSoup) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∑–∞ —Å–µ–≥–æ–¥–Ω—è"""
        views_regex = re.compile(r'(\d+)\s+–ø—Ä–æ—Å–º–æ—Ç—Ä')
        views_block = soup.find('div', string=views_regex)
        
        if not views_block:
            links = soup.find_all('a', string=views_regex)
            if links:
                views_block = links[0]
                
        if views_block:
            text = views_block.get_text(strip=True)
            today_match = re.search(r'(\d+)\s+–∑–∞\s+—Å–µ–≥–æ–¥–Ω—è', text)
            if today_match:
                return int(today_match.group(1))
            return 0
        return None

    def _extract_rooms(self, title: str, soup: BeautifulSoup) -> Optional[int]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç"""
        # –ò–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        match = re.search(r'(\d+)-–∫–æ–º–Ω', title)
        if match:
            return int(match.group(1))
        if '—Å—Ç—É–¥–∏—è' in title.lower():
            return 0
            
        # –ò–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        fact_items = soup.find_all('div', {'data-name': 'OfferFactItem'})
        for item in fact_items:
            label = item.find('div', class_=re.compile(r'--title--'))
            if label and '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç' in label.get_text(strip=True).lower():
                value = item.find('div', class_=re.compile(r'--value--'))
                if value:
                    v_text = value.get_text(strip=True)
                    if '—Å—Ç—É–¥–∏—è' in v_text.lower():
                        return 0
                    m = re.search(r'(\d+)', v_text)
                    if m:
                        return int(m.group(1))
        return None

    def _extract_area(self, title: str, soup: BeautifulSoup) -> Optional[float]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ–±—â—É—é –ø–ª–æ—â–∞–¥—å"""
        # –ò–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        match = re.search(r'(\d+[\.,]?\d*)\s*–º¬≤', title)
        if match:
            return float(match.group(1).replace(',', '.'))
            
        # –ò–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        fact_items = soup.find_all('div', {'data-name': 'OfferFactItem'})
        for item in fact_items:
            label = item.find('div', class_=re.compile(r'--title--'))
            if label and '–æ–±—â–∞—è –ø–ª–æ—â–∞–¥—å' in label.get_text(strip=True).lower():
                value = item.find('div', class_=re.compile(r'--value--'))
                if value:
                    v_text = value.get_text(strip=True)
                    m = re.search(r'(\d+[\.,]?\d*)', v_text)
                    if m:
                        return float(m.group(1).replace(',', '.'))
        return None

    async def _deep_parse(self, url: str) -> Optional[Dict[str, Any]]:
        """–ì–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö"""
        html_content = await self._fetch_page_content(url)
        if not html_content:
            return None
            
        soup = BeautifulSoup(html_content, 'lxml')
        page_text = soup.get_text().lower()
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        auction_keywords = ['–∞—É–∫—Ü–∏–æ–Ω', 'auction', '—Ç–æ—Ä–≥–∏', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ']
        deposit_keywords = ['–∑–∞–ª–æ–≥', '–≤–Ω–µ—Å–µ–Ω –∑–∞–ª–æ–≥', '–≤–Ω–µ—Å—ë–Ω –∑–∞–ª–æ–≥', '–¥–µ–ø–æ–∑–∏—Ç', 'deposit', '–∞–≤–∞–Ω—Å']
        for kw in (auction_keywords + deposit_keywords):
            if kw in page_text:
                if len(kw) > 4:
                    logger.info(f"Skipping {url}: found keyword '{kw}'")
                    return None

        address_elem = soup.find('div', {'data-name': 'Geo'})
        address = address_elem.get_text(strip=True) if address_elem else "Unknown Address"
        
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–º–Ω–∞—Ç/–ø–ª–æ—â–∞–¥–∏
        page_title_elem = soup.find('h1', {'data-name': 'OfferTitle'})
        page_title = page_title_elem.get_text(strip=True) if page_title_elem else ""
        
        floor, floors_total = self._extract_floor_info(soup)
        
        return {
            'address': address,
            'metro_stations': self._extract_metro_info(soup),
            'floor': floor,
            'floors_total': floors_total,
            'views_today': self._extract_views(soup),
            'rooms': self._extract_rooms(page_title, soup),
            'area': self._extract_area(page_title, soup)
        }

    async def parse_page(self, url: str) -> List[List[Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
        logger.info(f"Parsing page: {url}")
        html_content = await self._fetch_page_content(url)
        if not html_content:
            return []

        soup = BeautifulSoup(html_content, 'lxml')
        cards = soup.find_all('article', {'data-name': 'CardComponent'})
        if not cards:
             cards = soup.find_all('div', class_=re.compile(r'--card--'))

        if not cards:
             cards = soup.find_all('div', class_=re.compile(r'--card--'))
        
        if not cards:
            logger.warning(f"‚ö†Ô∏è 0 cards found! Page Title: {soup.title.string if soup.title else 'No Title'}")
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã (–∫–∞–ø—á–∞, –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–µ—Ä—Å—Ç–∫–∏)
            logger.info(f"Page content dump (first 500 chars): {soup.get_text()[:500]}")

        logger.info(f"Found {len(cards)} cards on page")
        
        apartments = []
        tasks = []
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∏–≥–≥–µ—Ä–∏—Ç—å –∫–∞–ø—á—É
        semaphore = asyncio.Semaphore(2) 
        
        # –°–Ω–∞—á–∞–ª–∞ —Å–æ–±–∏—Ä–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        for card in cards:
            try:
                # –ü–æ–ø—ã—Ç–∫–∞ 1: –ü–æ–∏—Å–∫ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é –∫–ª–∞—Å—Å–∞ (–±–æ–ª–µ–µ –º—è–≥–∫–∏–π —Ä–µ–≥–µ–∫—Å)
                link_elem = card.find('a', class_=re.compile(r'--link'))
                
                # –ü–æ–ø—ã—Ç–∫–∞ 2: –ü–æ–∏—Å–∫ –ø–æ href (fallback), –µ—Å–ª–∏ –∫–ª–∞—Å—Å –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –∏–∑–º–µ–Ω–∏–ª—Å—è
                if not link_elem:
                    link_elem = card.find('a', href=re.compile(r'/flat/|/sale/flat/'))
                
                if not link_elem:
                    # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ HTML –∫–∞—Ä—Ç–æ—á–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏, –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞
                    logger.warning(f"‚ö†Ô∏è Link not found for card. Content start: {str(card)[:100]}...")
                    continue

                href = link_elem.get('href')
                if not href: continue
                
                title_elem = card.find('span', {'data-mark': 'OfferTitle'}) or card.find('div', class_=re.compile(r'--title--'))
                title = title_elem.get_text(strip=True) if title_elem else "No Title"

                # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
                if any(kw in title.lower() for kw in ['–∞—É–∫—Ü–∏–æ–Ω', 'auction', '—Ç–æ—Ä–≥–∏', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ']):
                    continue

                price_elem = card.find('span', {'data-mark': 'MainPrice'}) or card.find('div', class_=re.compile(r'--price--'))
                price_str = price_elem.get_text(strip=True) if price_elem else "0"

                psqm_elem = card.find('p', {'data-mark': 'PriceInfo'})
                price_per_sqm = psqm_elem.get_text(strip=True) if psqm_elem else ""

                # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á—É –Ω–∞ –≥–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
                tasks.append(self._process_card(href, title, price_str, price_per_sqm, semaphore))
                
            except Exception as e:
                logger.error(f"Error parsing card basic info: {e}")

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–ª—É–±–æ–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks)
        for res in results:
            if res:
                apartments.append(res)

        return apartments

    async def _process_card(self, href, title, price_str, price_per_sqm, semaphore):
        """–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞—Ä—Ç–æ—á–∫–∏ —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º"""
        async with semaphore:
            try:
                # –ë–æ–ª–µ–µ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                wait_time = random.uniform(config.MIN_PARSER_DELAY / 2, config.MAX_PARSER_DELAY / 2)
                logger.debug(f"Waiting {wait_time:.1f}s before deep parse of {href}")
                await asyncio.sleep(wait_time)
                
                details = await self._deep_parse(href)
                if details:
                    return [href, title, price_str, price_per_sqm, details]
            except Exception as e:
                logger.error(f"Deep parse error for {href}: {e}")
                return [href, title, price_str, price_per_sqm, {}]
        return None
